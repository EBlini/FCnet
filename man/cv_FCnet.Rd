% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cv_FCnet.R
\name{cv_FCnet}
\alias{cv_FCnet}
\title{CrossValidate FCnet model}
\usage{
cv_FCnet(
  y,
  x,
  alpha = seq(0, 1, by = 0.1),
  lambda = rev(10^seq(-5, 5, length.out = 200)),
  nfolds = nrow(x),
  rep_cv = 1,
  type.measure = optionsFCnet("cv.type.measure"),
  intercept = optionsFCnet("intercept"),
  standardize = optionsFCnet("standardize"),
  ...
)
}
\arguments{
\item{y}{The dependent variable, typically behavioral scores to predict.
This can be a vector or a single data.frame column.}

\item{x}{The independent variables, typically neural measures that have
been already summarised through data reduction techniques
(e.g. ICA, PCA): an object created by \code{reduce_featuresFC()} will do. If such
an object is passed to this function, the "Weights" slot is taken as x.
A list can be passed to this function: in this case the function needs an
entry named "Weights". Otherwise, a data.frame can be passed to x.}

\item{alpha}{Value(s) that bias the elastic net toward ridge regression
(alpha== 0) or LASSO regression (alpha== 1). If a vector of alpha values
is supplied, the value is optimized through crossvalidation.
It defaults to a vector ranging from 0 to 1 with steps of 0.1.
The crossvalidated alpha is returned.}

\item{lambda}{Regularization parameter for the regression,
see \code{glmnet::glmnet()}. Lambda must be a vector with length>1.
When a vector of lambda values is supplied, the value of lambda
is optimized through internal crossvalidation. It defaults to a vector
ranging from 10^-5 to 10^5 with 200 values in logarithmic steps.
The crossvalidated optimal lambda is returned.}

\item{nfolds}{Number of folds to be created in the crossvalidation of alpha
and/or lambda. It defaults to \code{nrow(x)}, that is Leave-One-Out crossvalidation.
It can be set to whatever integer >3 and < nrow(x).}

\item{rep_cv}{Number of times the crossvalidation procedure must be repeated.
It defaults to 1 (as the LOO procedure is deterministic, thus values larger
than 1 are redundant). If rep_cv>1, the crossvalidation is repeated rep_cv
times, and the consensus (by default: median) crossvalidated alpha and lambda values across all rep_cv
are returned. Useful in order to decrease randomness when kfold crossvalidation
is required. Setting this parameter too high though will result in a mere
approximation of the LOO.}

\item{intercept}{whether to fit (TRUE) or not (FALSE) an intercept to the model.}

\item{standardize}{Whether x must be standardized internally to glmnet.}

\item{...}{Other parameters passed to \code{glmnetUtils::cva.glmnet()}.}

\item{cv.type.measure}{The measure to minimize in crossvalidation inner loops.
Differently from \code{glmnetUtils::cva.glmnet()} the deafult is the mean absolute error.}
}
\value{
The crossvalidated alpha and lambda parameters with the associated error.
}
\description{
This function is a wrapper around \code{glmnetUtils::cva.glmnet()}, which is
itself built around \code{glmnet::cv.glmnet()}. For extended documentation,
the readers are encouraged to consult the respective pages.
\code{cv_FCnet()} requires two objects at minimum: \code{y} is a vector or data.frame
with exactly one column, corresponding to the (behavioral) score to predict; \code{x}
is a data.frame or a list of lists with an entry named "Weights",
which includes the independent variables. \code{x} can be - and is meant to be -
one object created by \code{reduce_featuresFC()}, but this is not strictly necessary.
The crossvalidated lambda and alpha parameters are returned. For the model,
use the \code{FCnetLOO()} function to avoid overfitting.
Differently from \code{glmnet::cv.glmnet()}, here the
mean absolute error is minimized by default; you can change this parameter
directly in the function or by running \code{optionsFCnet(optionsFCnet= "mse")}.
}
