% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cv_FCnet.R
\name{cv_FCnet}
\alias{cv_FCnet}
\title{CrossValidate FCnet model}
\usage{
cv_FCnet(
  y,
  x,
  alpha = seq(0, 1, by = 0.1),
  lambda = rev(10^seq(-5, 5, length.out = 200)),
  cv_Ncomp = NULL,
  cv_Ncomp_method = c("order", "R"),
  type.measure = optionsFCnet("cv.type.measure"),
  intercept = optionsFCnet("intercept"),
  standardize = optionsFCnet("standardize"),
  thresh = optionsFCnet("thresh"),
  ...
)
}
\arguments{
\item{y}{The dependent variable, typically behavioral scores to predict.
This can be a vector or a single data.frame column.}

\item{x}{The independent variables, typically neural measures that have
been already summarised through data reduction techniques
(e.g. ICA, PCA): an object created by \code{reduce_featuresFC()} will do. If such
an object is passed to this function, the "Weights" slot is taken as x.
A list can be passed to this function: in this case the function needs an
entry named "Weights". Otherwise, a data.frame can be passed to x.}

\item{alpha}{Value(s) that bias the elastic net toward ridge regression
(alpha== 0) or LASSO regression (alpha== 1). If a vector of alpha values
is supplied, the value is optimized through crossvalidation.
It defaults to a vector ranging from 0 to 1 with steps of 0.1.
The crossvalidated alpha is returned.}

\item{lambda}{Regularization parameter for the regression,
see \code{glmnet::glmnet()}. Lambda must be a vector with length>1.
When a vector of lambda values is supplied, the value of lambda
is optimized through internal crossvalidation. It defaults to a vector
ranging from 10^-5 to 10^5 with 200 values in logarithmic steps.
The crossvalidated optimal lambda is returned.}

\item{cv_Ncomp}{Whether to crossvalidate the number of components or not.
It defaults to NULL, but a vector can be supplied specifing the number (range) of
components to test in the inner loops.}

\item{cv_Ncomp_method}{Whether the number of components to optimize means
components are ordered (e.g. according to the explained variance of neuroimaging
data) or - somehow experimental - whether to use the N best components
ranked according to their relationship (pearson's R) with y.}

\item{intercept}{whether to fit (TRUE) or not (FALSE) an intercept to the model.}

\item{standardize}{Whether x must be standardized internally to glmnet.}

\item{thresh}{Threshold for glmnet to stop converging to the solution.}

\item{...}{Other parameters passed to \code{glmnetUtils::cva.glmnet()}.}

\item{cv.type.measure}{The measure to minimize in crossvalidation inner loops.
Differently from \code{glmnetUtils::cva.glmnet()} the deafult is the mean absolute error.}
}
\value{
The crossvalidated alpha and lambda parameters with the associated error.
}
\description{
This function is a wrapper around \code{glmnetUtils::cva.glmnet()}, which is
itself built around \code{glmnet::cv.glmnet()}. For extended documentation,
the readers are encouraged to consult the respective pages.
\code{cv_FCnet()} requires two objects at minimum: \code{y} is a vector or data.frame
with exactly one column, corresponding to the (behavioral) score to predict; \code{x}
is a data.frame or a list of lists with an entry named "Weights",
which includes the independent variables. \code{x} can be - and is meant to be -
one object created by \code{reduce_featuresFC()}, but this is not strictly necessary.
The crossvalidated lambda and alpha parameters are returned. For the model,
use the \code{FCnetLOO()} function to avoid overfitting.
Differently from \code{glmnet::cv.glmnet()}, here the
mean absolute error is minimized by default; you can change this parameter
directly in the function or by running \code{optionsFCnet(optionsFCnet= "mse")}.
}
