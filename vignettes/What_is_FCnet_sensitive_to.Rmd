---
title: "What_is_FCnet_sensitive_to"
author: "Elvio Blini"
date: "December 2020"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{What_is_FCnet_sensitive_to}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(width = 999)

```

## Introduction

The analysis of (Functional Connectivity) neuroimaging data can be daunting due to the very high dimensionality of the features involved. 
In time, several approaches to the problem have been devised. `FCnet` allows one to easily implement a two steps procedure consisting of:


  1) **Feature reduction**: the functional connectivity matrices are first summarized through data reduction techniques such as Principal Component Analysis or Independent Components Analysis. 
 
 
  2) **Robust regression**: the reduced matrix of Weights is then entered into a robust regression model (with either ridge or LASSO penalty). The model is crossvalidated internally by means of Leave-One-Out crossvalidation.
  
  
For useful references, see: [Siegel et al., 2016](https://www.pnas.org/content/113/30/E4367); [Salvalaggio et al., 2020](https://academic.oup.com/brain/article/143/7/2173/5861020);
 [Calesella et al., 2020](https://link.springer.com/chapter/10.1007%2F978-3-030-59277-6_3).
 
 This vignette will provide an overview of `FCnet` main routines and try to understand to which changes in the FC matrices the approach it implements is most sensitive to. 

## Setting up the simulation

We will need the following packages:

```{r}
library("FCnet") #for the main analysis routines
library("ggplot2") #beautiful depictions
library("gridExtra") #arrange plots

```

The `FCnet` package comes with a 324x324 matrix of functional connectivity data obtained from 37 participants. We use it as a starting point to simulate several FC matrices.
Please note that the functions - written in full below - are available in `FCnet` (see `?simulateFCnet` for help), but are therein subject to changes.

```{r}
data("MeanFC")

#this function restores FC matrices that have been messed with
polishMat= function(mat){
  
  #copy upper triangle to lower triangle 
  mat[lower.tri(mat)]=  t(mat)[lower.tri(mat)]
  
  #set to 1 correlations larger than 1
  mat[mat>1]= 1
  mat[mat<-1]= -1
  
  #diagonal back to 1
  diag(mat)= 1
  
  return(mat)
  
}

#this function creates several matrices by adding gaussian noise
#with sd "variability" to a reference matrix
simulateMat= function(mat,
                      Nmat,
                      variability){
  
  lapply(1:Nmat, function(s){
    
    ms= mat + rnorm(nrow(mat)*ncol(mat), 
                    mean = 0, sd= variability)
    ms= polishMat(ms)              
    
  })
}

set.seed(1)

N_subs= 50 #number of participants/matrices
Subjs_variability= 0.2 #variability between matrices

m_start= simulateMat(MeanFC, N_subs, Subjs_variability)

```

`m_start` is now a list of lists, the preferred format of `FCnet`, including several FC matrices.
Now we perturb only one subset of these matrices (i.e. a network) in one of two ways:


  1) **Bias:** we bias one network systematically as a function of one behavioral score `y` (to be predicted). In this scenario, a subset of edges will be injected with signal directly related to the behavioral score to be ultimately predicted: these edges will be correlated with the score `y`. To have a more realistic simulation, this bias will not be constant but rather modulated by a random factor such that different participants may present different degrees of this bias.
  
  
  2) **Noise:** in this scenario, gaussian noise - but not a systematic bias - will be injected into a specific network. The noise will be proportional to the behavioral score to predict `y` (lower noise for participants performing better, larger noise for participants performing worse).
  
We initialize these parameters and set up the functions. For the bias:

```{r}
y= rnorm(N_subs, mean = 0, sd= 1) #the behavioral score

network1= 50:100 #network to perturb
network2= 200:250 #network to perturb

bias_multiplier= 0.15
bias_variability= 0.05
noise_multiplier= 0.15

#this function biases a given network as a function 
#of a behavioral score y
biasMat= function(matrices, y, 
                  network1, network2, 
                  bias_multiplier, bias_variability){
  
  lapply(1:length(matrices), function(x){
    
    mat= matrices[[x]]
    
    #the score will be proportional to participants' performance
    #though with a random component included
    score= (y[x] / max(abs(y))) * 
      (bias_multiplier + rnorm(1, 0, bias_variability))
    
    dim_mat= dim(mat[network1, network2])
    
    mat[network1, network2] = mat[network1, network2] +
      rep(score, dim_mat[1]*dim_mat[2])
    mat[network2, network1] = mat[network2, network1] +
      rep(score, dim_mat[1]*dim_mat[2])
    
    mat= polishMat(mat)
    
    return(mat)
    
    })
 
}  

m_bias= biasMat(m_start, y, network1, network2, 
                bias_multiplier, bias_variability)
  
```

For the noise:

```{r}
#this function adds gaussian noise to a network
#amount of noise is proportional to y
noiseMat= function(matrices, y,
                   network1, network2,
                   noise_multiplier){
  
  lapply(1:length(matrices), function(x){
    
    mat= matrices[[x]]
    
    #variabilitry of the score (must be >0) function of y
    score= (y[x] + abs(min(y)))/max(abs(y)) + 0.001
    
    dim_mat= dim(mat[network1, network2])
    
    mat[network1, network2] = mat[network1, network2] +
      rnorm(dim_mat[1]*dim_mat[2], mean= 0, sd = score*noise_multiplier)
    
    mat[network2, network1] = mat[network2, network1] +
      rnorm(dim_mat[1]*dim_mat[2], mean= 0, sd = score*noise_multiplier)
    
    mat= polishMat(mat)
    
    return(mat)
    
    })
 
} 

m_noise= noiseMat(m_start, y, network1, network2, noise_multiplier)

```

We now have three sets of matrices: the starting set `m_start`, in which no signal has been added; the biased set `m_bias`, in which connections in a given subnetwork have been biased according to y; the perturbed set `m_noise`, in which the same network has been perturbed with gaussian noise depending on y.

We can plot a representative participant with the utility function `plotFC`:

```{r fig.height=12, fig.width= 8}
grid_net= list(geom_rect(aes(xmin= min(network1), 
                         xmax= max(network1),
                         ymin= min(network2),
                         ymax= max(network2)),
                         size= 1.05, color= "black", 
                         fill= "transparent"))

ps= plotFC(m_start[[which.max(y)]], 
           lim= c(-1, 1)) + ggtitle("Start") + grid_net

pb= plotFC(m_bias[[which.max(y)]], 
           lim= c(-1, 1)) + ggtitle("Bias") + grid_net

pn= plotFC(m_start[[which.max(y)]], 
           lim= c(-1, 1)) + ggtitle("Noise") + grid_net

grid.arrange(ps, pb, pn)

```

## Features reduction

Once we have obtained all the simulated matrices, we can chain all the `FCnet` pipeline in order to see whether models are predictive or not, and whether we can reconstruct the network that we have artificially perturbed or biased.

We start with performing feature reduction through PCA.

```{r}
rf_start= reduce_featuresFC(m_start) 
rf_bias= reduce_featuresFC(m_bias) 
rf_noise= reduce_featuresFC(m_noise) 

```

We can now check the mean FC matrices: i) mean matrices do not differ between starting matrices and matrices with systematic bias introduced, because the network was biased via a standardized dependent variable (i.e. the mean of the bias was zero across participants). 
We should not be able to appreciate differences in the mean matrices between `m_start` and `m_bias` mean matrices.

```{r fig.height=12, fig.width= 8}
ps= plotFC(rf_start$MeanFC, lim= c(-1, 1)) + 
  ggtitle("Start") + grid_net
pb= plotFC(rf_bias$MeanFC, lim= c(-1, 1)) + 
  ggtitle("Bias") + grid_net
pn= plotFC(rf_noise$MeanFC, lim= c(-1, 1)) + 
  ggtitle("Noise") + grid_net

grid.arrange(ps, pb, pn)

```

We then can check that, on the other hand, matrices differ between starting matrices and matrices with noise, as per design, in terms of variability. We should be able to appreciate differences in the standard deviation of matrices' entries between `m_start` and `m_noise` matrices.

```{r fig.height=12, fig.width= 8}
f_sd= function(lst){
  
  n= length(lst) 	   
  rc= dim(lst[[1]]) 	   
  ar1= array(unlist(lst), c(rc, n)) 	
  
  apply(ar1, c(1, 2), function(x)(sd(x, na.rm= T))) 	         

}
sd_start= f_sd(m_start)
sd_bias= f_sd(m_bias)
sd_noise= f_sd(m_noise)

ps= plotFC(sd_start, lim= c(0, 0.3)) + ggtitle("Start") + grid_net
pb= plotFC(sd_bias, lim= c(0, 0.3)) + ggtitle("Bias") + grid_net
pn= plotFC(sd_noise, lim= c(0, 0.3)) + ggtitle("Noise") + grid_net

grid.arrange(ps, pb, pn)

```

All our assumptions appear to be, more or less, met!

## Modelling

We can run Leave-One-Out cross-validated robust regressions with one line:

```{r}
r_start= FCnetLOO(y, rf_start, alpha= 0)
r_start$R2

r_bias= FCnetLOO(y, rf_bias, alpha= 0)
r_bias$R2

r_noise= FCnetLOO(y, rf_noise, alpha= 0)
r_noise$R2

```

It appears that the first model - totally random - is capable to explain about 10% of the variance from LOO observations. (This is probably non-significant if probed with a permutation test) 
The model in which a systematic perturbation of the edges made them correlated with the score to predict appears to be sensibly more predictive.
Finally, the model in which noise has been added - proportionally to the behavioral score to predict - perfiorms slightly better than the starting model.

A far more compelling proof, however, will be back-projection. Ideally, relevant edges will be scattered all around the matrix for non-predictive models, but will be centered on the network we perturbed for really informative ones.

## Visualization

Next, we ensure that back-projected coefficients properly identify the network that we perturbed:

```{r fig.height=12, fig.width= 8}
ps= backprojectFCnet(r_start, rf_start, 
                     threshold = length(network1)*length(network2)) 
ps= plotFC(ps) + ggtitle("Start") + grid_net

pb= backprojectFCnet(r_bias, rf_bias, 
                     threshold = length(network1)*length(network2)) 
pb= plotFC(pb) + ggtitle("Bias") + grid_net

pn= backprojectFCnet(r_noise, rf_noise, 
                     threshold = length(network1)*length(network2)) 
pn= plotFC(pn) + ggtitle("Noise") + grid_net

grid.arrange(ps, pb, pn)

```

Indeed, predictive edges are scattered all over the places for `start` matrices. `bias` and, to a lesser extent, `noise` matrices, on the other hand, appear to highlight the exact same network we perturbed as the most relevant one. You can also tell by the scale of coefficients (which is minuscule for the starting matrix, but larger for biased and perturbed matrices).

## Conclusion

These simulations were based on crude assumptions and parameters. Yet, it appears that, in principle, the procedures in `FCnet` may help recovering the most predictive connections from very rich and multidimensional datasets. 
Procedures appear to be sensitive to both bias and noise. 
Sensitivity to systematic drifts appears interesting for studies on healthy participants and behavioral scores, for which the underlying hypothesis is that stronger network connections (e.g. training) may drive a better performance.
Sensitivity to the noise within FC matrices, in addition, appears especially intriguing for studies on clinical populations and "impairment scores". In these populations, network disturbance may be a further source of behavioral impairment. 
More systematic simulations will help further shedding light on this issue.

## Appendix

Packages' versions:

```{r}
sessionInfo()

```

