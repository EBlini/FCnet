---
title: "What is FCnet sensitive to?"
author: "Elvio Blini"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{What is FCnet sensitive to?}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = normalizePath('./'))
options(width = 999)
```

# What is FCnet sensitive to?

The analysis of (Functional Connectivity) neuroimaging data can be daunting due to the very high dimensionality of the features involved. 
In time, several approaches to the problem have been devised. `FCnet` allows one to easily implement a two steps procedure consisting of:
  1) **Feature reduction**: the functional connectivity matrices are first summarized through data reduction techniques such as Principal Component Analysis or Independent Components Analysis. 
  2) **Robust regression**: the reduced matrix of Weights is then entered into a robust regression model (with either ridge or LASSO penalty). The model is crossvalidated internally by means of Leave-One-Out crossvalidation.
  
For useful references, see: [Siegel et al., 2016](https://www.pnas.org/content/113/30/E4367); [Salvalaggio et al., 2020](https://academic.oup.com/brain/article/143/7/2173/5861020);
 [Calesella et al., 2020](https://link.springer.com/chapter/10.1007%2F978-3-030-59277-6_3).
 
 This vignette will provide an overview of `FCnet` main routines and try to understand to which changes in the FC matrices the approach it implements is most sensitive to. 

# Setting up the simulation

We will need the following packages:

```{r}
library("FCnet")
library("magrittr")
library("ggplot2")
library("patchwork")

```

The `FCnet` package comes with a 324x324 matrix of functional connectivity data obtained from 37 participants. We use it as a starting point to simulate 25 FC matrices.

```{r}
data("MeanFC")

polishMat= function(mat){
  
  #copy upper triangle to lower triangle 
  mat[lower.tri(mat)]=  t(mat)[lower.tri(mat)]
  
  #set to 1 correlations larger than 1
  mat[mat>1]= 1
  mat[mat<-1]= -1
  
  return(mat)
  
}

simulateMat= function(mat,
                      Nmat,
                      variability){
  
  lapply(1:Nmat, function(s){
    
    ms= mat + rnorm(nrow(mat)*ncol(mat), mean = 0, sd= variability)
    ms= polishMat(ms)              
    
  })
}

set.seed(1)

N_subs= 25 #number of participants/matrices
Subjs_variability= 0.2 #variability between matrices

m_start= simulateMat(MeanFC, N_subs, Subjs_variability)

```

`m_start` is now a list of lists, the preferred format of `FCnet`, including several FC matrices.
Now we perturb only one subset of these matrices (i.e. a network) in one of two ways:
  1) **Bias:** we bias one network systematically as a function of one behavioral score `y` (to be predicted). In this scenario, a subset of edges will be injected with signal directly related to the behavior score to be ultimately predicted: these edges will be correlated with the score `y`.
  2) **Noise:** in this scenario, gaussian noise - but not a systematic bias - will be injected into a specific network. The noise will be proportional to the behavioral score to predict `y` (lower noise for participants performing better, larger noise for participants performing worse).
  
We initialize these parameters and set up the functions. For the bias:

```{r}
y= rnorm(N_subs, mean = 0, sd= 1) #â˜»behavioral score

network1= 50:100 #network to perturb
network2= 200:250 #network to perturb

bias_multiplier= 0.1
noise_multiplier= 0.15

biasMat= function(matrices, y, 
                  network1, network2, 
                  bias_multiplier){
  
  lapply(1:length(matrices), function(x){
    
    mat= matrices[[x]]
    
    score= y[x] / max(y)
    
    dim_mat= dim(mat[network1, network2])
    
    mat[network1, network2] = mat[network1, network2] +
      rep(score*bias_multiplier, dim_mat[1]*dim_mat[2])
    mat[network2, network1] = mat[network2, network1] +
      rep(score*bias_multiplier, dim_mat[1]*dim_mat[2])
    mat= polishMat(mat)
    
    return(mat)
    
    })
 
}  

m_bias= biasMat(m_start, y, network1, network2, bias_multiplier)
  
```

For the noise:

```{r}
noiseMat= function(matrices, y,
                   network1, network2,
                   noise_multiplier){
  
  lapply(1:length(matrices), function(x){
    
    mat= matrices[[x]]
    
    score= (y[x] + abs(min(y)))/max(abs(y)) + 0.001
    
    dim_mat= dim(mat[network1, network2])
    
    mat[network1, network2] = mat[network1, network2] +
      rnorm(dim_mat[1]*dim_mat[2], mean= 0, sd = score*noise_multiplier)
    
    mat[network2, network1] = mat[network2, network1] +
      rnorm(dim_mat[1]*dim_mat[2], mean= 0, sd = score*noise_multiplier)
    
    mat= polishMat(mat)
    
    return(mat)
    
    })
 
} 

m_noise= noiseMat(m_start, y, network1, network2, noise_multiplier)

```

We now have three sets of matrices: the starting set `m_start`, in which no signal has been added; the biased set `m_bias`, in which connections in a given subnetwork have been biased according to y; the perturbed set `m_noise`, in which the same network has been perturbed with gaussian noise depending on y.

We can plot a representative participant with the utility function `plotFC`:

```{r fig.height=9, fig.width= 9}
#which.max(y) #id 19 should have more noise and more correlated scores

grid_net= list(geom_rect(aes(xmin= min(network1), 
                         xmax= max(network1),
                         ymin= min(network2),
                         ymax= max(network2)),
                         size= 1.05, color= "black", 
                         fill= "transparent"))

ps= plotFC(m_start[[19]], lim= c(-1, 1)) + ggtitle("Start") + grid_net

pb= plotFC(m_bias[[19]], lim= c(-1, 1)) + ggtitle("Bias") + grid_net

pn= plotFC(m_start[[19]], lim= c(-1, 1)) + ggtitle("Noise") + grid_net

(ps)/ (pb + pn)

```

# Features reduction

Once we have obtained all the simulated matrices, we can chain all the `FCnet` pipeline in order to see whether models are predictive or not, and whether we can reconstruct the network that we have artifically perturbed or biased.

We start with performing feature reduction through PCA.

```{r}
rf_start= reduce_featuresFC(m_start) 
rf_bias= reduce_featuresFC(m_bias) 
rf_noise= reduce_featuresFC(m_noise) 

```

We can check that: i) mean matrices differ between starting matrices and matrices with systematic bias introduced, as per designed. We should be able to appreciate differences in the mean matrices between `m_start` and `m_bias` mean matrices.

```{r fig.height=9, fig.width= 9}
ps= plotFC(rf_start$MeanFC, lim= c(-1, 1)) + 
  ggtitle("Start") + grid_net
pb= plotFC(rf_bias$MeanFC, lim= c(-1, 1)) + 
  ggtitle("Bias") + grid_net
pn= plotFC(rf_noise$MeanFC, lim= c(-1, 1)) + 
  ggtitle("Noise") + grid_net

(ps)/ (pb + pn)

```

We then can check that: ii) matrices differ between starting matrices and matrices with noise, as per designed. We should be able to appreciate differences in the standard deviation of matrices' entries between `m_start` and `m_noise` matrices.

```{r fig.height=9, fig.width= 9}
f_sd= function(lst){
  
  n= length(lst) 	   
  rc= dim(lst[[1]]) 	   
  ar1= array(unlist(lst), c(rc, n)) 	
  
  apply(ar1, c(1, 2), function(x)(sd(x, na.rm= T))) 	         

}
sd_start= f_sd(m_start)
sd_bias= f_sd(m_bias)
sd_noise= f_sd(m_noise)

ps= plotFC(sd_start, lim= c(0, 0.6)) + ggtitle("Start") + grid_net
pb= plotFC(sd_bias, lim= c(0, 0.6)) + ggtitle("Bias") + grid_net
pn= plotFC(sd_noise, lim= c(0, 0.6)) + ggtitle("Noise") + grid_net

(ps)/ (pb + pn)

```

All our assumptions appear to be, more or less, met!

# Modelling

We can run Leave-One-Out cross-validated robust regressions with one line:

```{r}
r_start= FCnetLOO(y, rf_start, alpha= 0)
r_start$R2

r_bias= FCnetLOO(y, rf_bias, alpha= 0)
r_bias$R2

r_noise= FCnetLOO(y, rf_noise, alpha= 0)
r_noise$R2

```

It appears that the first model - totally random - is capable to explain about 20% of the variance from LOO observations. (This is probably non-significant if probed with a permutation test) 
The model in which a systematic perturbation of the edges made them correlated with the score to predict - as expected - achieves a performance which is almost at ceiling.
Finally, the model in which noise has been added - proportionally to the behavioral score to predict - achieves poorly.

A far more compelling proof, however, will be back-projection. Ideally, relevant edges will be scattered all around the matrix for non-predictive models but centered on the network we perturbed for really informative ones.

# Visualization

Next, we ensure that back-projected coefficients properly identify the network that we perturbed:

```{r fig.height=9, fig.width= 9}
ps= backprojectFCnet(r_start, rf_start, 
                     threshold = length(network1)*length(network2)) %>%
  plotFC() + ggtitle("Start") + grid_net

pb= backprojectFCnet(r_bias, rf_bias, 
                     threshold = length(network1)*length(network2)) %>%
  plotFC() + ggtitle("Bias") + grid_net

pn= backprojectFCnet(r_noise, rf_noise, 
                     threshold = length(network1)*length(network2)) %>%
  plotFC() + ggtitle("Noise") + grid_net

(ps)/ (pb + pn)

```

Indeed, predictive edges are scattered all over the places for `start` and `noise` matrices. `bias` matrices, on the other hand, appear to highlight the exact same network we perturbed as the most relevant one.

# Appendix

Packages' versions:

```{r}
sessionInfo()

```

